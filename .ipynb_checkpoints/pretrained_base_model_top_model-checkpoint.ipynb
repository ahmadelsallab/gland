{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "import keras.utils as np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load training data frame\n",
    "#train_data_path = '../../dat/train.csv'\n",
    "train_data_path = 'C:/Non_valeo/Research/PostDoc/Kaggle/GLand/dat/train.csv'\n",
    "all_train_df = pd.read_csv(train_data_path)\n",
    "print('Training DataFrame loaded')\n",
    "all_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = all_train_df.loc[all_train_df.id =='cacf8152e2d2ae60']['landmark_id']\n",
    "#all_train_df.loc[all_train_df.id =='cacf8152e2d2ae60', ['landmark_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = 400\n",
    "L = 400\n",
    "N_CH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(directory, df):\n",
    "\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    # Loop on all jpg files\n",
    "    for img_file in os.listdir(directory):\n",
    "\n",
    "        try:\n",
    "            img = Image.open(os.path.join(directory, img_file))\n",
    "            #img = np.array(img.resize((L, W, N_CH)))\n",
    "            img = np.resize(img, (L,W,N_CH))\n",
    "            x.append(img)\n",
    "            id_str = os.path.splitext(img_file)[0]\n",
    "            y.append(int(df.loc[df.id ==id_str]['landmark_id']))\n",
    "        except:\n",
    "            print('Cannot load img file')\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#jpgfile = Image.open(all_train_df.iloc[0]['id']+'.jpg')\n",
    "#jpgfile = Image.open(open(\"../../dat/all_train_raw/0a58358a2afd3e4e.jpg\", 'rb'))\n",
    "\n",
    "#img = Image.open(os.path.abspath(\"../../dat/all_train_raw/fb3bd13be65b86b0.jpg\"))\n",
    "img = Image.open(os.path.abspath(\"C:/Non_valeo/Research/PostDoc/Kaggle/GLand/dat/train/000a1fcfa0a7a9ac.jpg\"))\n",
    "\n",
    "#jpgfile.show()\n",
    "plt.imshow(img)\n",
    "\n",
    "#img\n",
    "\n",
    "\n",
    "w = 300\n",
    "h = 400\n",
    "n_ch = 3\n",
    "#img = img.resize((w,h), Image.ANTIALIAS)\n",
    "#img = img.resize((w,h))\n",
    "img = np.resize(img, (w,h,n_ch))\n",
    "\n",
    "img = np.array(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "\n",
    "\n",
    "#directory = '../../dat/all_train_raw'\n",
    "directory = 'C:/Non_valeo/Research/PostDoc/Kaggle/GLand/dat/train'\n",
    "x, y = load_data(directory, all_train_df)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "directory = 'C:/Non_valeo/Research/PostDoc/Kaggle/GLand/dat/train'\n",
    "x = []\n",
    "y = []\n",
    "# Loop on all jpg files\n",
    "for img_file in os.listdir(directory):\n",
    "\n",
    "    try:\n",
    "        img = Image.open(os.path.join(directory, img_file))\n",
    "        #img = np.array(img.resize((L,W)))\n",
    "        img = np.resize(img, (L,W,3))\n",
    "        x.append(img)\n",
    "        id_str = os.path.splitext(img_file)[0]\n",
    "        y.append(int(all_train_df.loc[df.id ==id_str]['landmark_id']))\n",
    "    except:\n",
    "        print('Cannot load img file')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_path = '../../dat/test.csv'\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "print('Test DataFrame loaded')\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = '../../dat/test_raw'\n",
    "x_test, y_test = load_data(directory, test_df)\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random split train into train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "y_val = np_utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histo of classes in train/va/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Top model pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Top model + top base layers fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width = W\n",
    "img_height = L\n",
    "\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# build the VGG16 network\n",
    "\n",
    "input_shape = (3,img_width,img_height)\n",
    "VGG_model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (img_width,img_height,3))\n",
    "print('Model loaded.')\n",
    "print(VGG_model.summary())\n",
    "'''\n",
    "model = Sequential()\n",
    "for layer in VGG_model.layers:\n",
    "    model.add(layer)\n",
    "'''\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "#top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model.add(Flatten(input_shape=VGG_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "#model.add(top_model)\n",
    "model = Model(inputs= VGG_model.input, outputs= top_model(VGG_model.output))\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_augmentation = True\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_val, y_val, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare submission"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
